{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The Evolution of the American Presidency #\n",
      "\n",
      "#### Renzo Lucioni, Kathy Lin, Sherrie Wang, Matt Moellman ####"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# special IPython command to prepare the notebook for matplotlib\n",
      "%matplotlib inline \n",
      "\n",
      "from fnmatch import fnmatch\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import requests\n",
      "from pattern import web\n",
      "import json\n",
      "import re\n",
      "import sys\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = True\n",
      "rcParams['axes.facecolor'] = '#eeeeee'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Insert introduction*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Data Collection ###\n",
      "\n",
      "We will gather our data by scraping the [data](http://www.presidency.ucsb.edu/data.php) and [documents](http://www.presidency.ucsb.edu/index_docs.php) web archives maintained by The American Presidency Project at UCSB. We'll start by scraping the State of the Union addresses in the documents archive."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_document(url):\n",
      "    \"Given a url of document in the archive, get president, year, and text. Returns dictionary.\"\n",
      "    html = requests.get(url).text\n",
      "    dom = web.Element(html)\n",
      "    data = {}\n",
      "    i = dom.by_tag('meta')[3]\n",
      "    data['president'] = i.attributes['content'].split(':')[0]\n",
      "    data['year'] = i.attributes['content'].split('-')[-1].split()[-1]\n",
      "    text = dom.by_tag('span.displaytext')[0].content\n",
      "    for p in dom.by_tag('p'):\n",
      "        t = p.content\n",
      "        if fnmatch(t, \"*<hr*\"):\n",
      "            text += t.split('<hr')[0]\n",
      "        elif fnmatch(t, \"*<div*\"):\n",
      "            text += t.split('<div')[0]\n",
      "        else:\n",
      "            text += p.content\n",
      "#     text = text.decode('utf-8', 'ignore')\n",
      "#     text = text.encode('utf-8')\n",
      "    # strip out any remaining HTML tags\n",
      "    text = re.sub('<[^>]*>', '', text)\n",
      "    # replace unicode emdashes with hyphens\n",
      "    text = re.sub('\\x97', ' - ', text)\n",
      "    # remove bracketed cues (e.g., [applause])\n",
      "    text = re.sub('\\[.*\\]', '', text)\n",
      "    # replace HTML emdash with hyphen\n",
      "    data['text'] = re.sub('&mdash;', ' - ', text)\n",
      "    return data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_all_sotus():\n",
      "    \"Visits website with links to all SOTUs. Scrapes each one. Writes JSON to file.\"\n",
      "    url = \"http://www.presidency.ucsb.edu/sou.php\"\n",
      "    html = requests.get(url).text\n",
      "    dom = web.Element(html)\n",
      "    sotus = []\n",
      "    for link in web.find_urls(dom):\n",
      "        if fnmatch(link, \"*pid*\"):\n",
      "            sotus.append(get_document(link))\n",
      "    \n",
      "    f = open('sotus.json', 'w')\n",
      "    f.write(json.dumps(sotus, indent=2))\n",
      "    f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# toggle to scrape and store SOTU data\n",
      "# get_all_sotus()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in State of the Union data\n",
      "f = open('sotus.json')\n",
      "raw = f.read()\n",
      "sotus = json.loads(raw)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll scrape the directory of Inaugural Addresses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_all_inaugurals():\n",
      "    \"Visits website with links to all inaugurals. Scrapes each one. Writes JSON to file.\"\n",
      "    url = \"http://www.presidency.ucsb.edu/inaugurals.php\"\n",
      "    html = requests.get(url).text\n",
      "    dom = web.Element(html)\n",
      "    inaugurals = []\n",
      "    for link in web.find_urls(dom):\n",
      "        if fnmatch(link, \"*pid*\"):\n",
      "            inaugurals.append(get_document(link))\n",
      "    \n",
      "    f = open('inaugurals.json', 'w')\n",
      "    f.write(json.dumps(inaugurals, indent=2))\n",
      "    f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# toggle to scrape and store Inaugural Address data\n",
      "# get_all_inaugurals()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in Inaugural Address data\n",
      "f = open('inaugurals.json')\n",
      "raw = f.read()\n",
      "inaugurals = json.loads(raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we'll scrape all Presidential Saturday Radio Addresses. These began with the Reagan Administration in 1982."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_all_saturday_addresses():\n",
      "    \"Visits website with links to all Saturday Radio Addresses. Scrapes each one. Writes JSON to file.\"\n",
      "    saturday_addresses = []\n",
      "    base_url = \"http://www.presidency.ucsb.edu/ws/index.php?pid=\"\n",
      "    for year in range(1982,2014):\n",
      "        print \"Scraping Saturday Radio Addresses from {}...\".format(year)\n",
      "        sys.stdout.flush()\n",
      "        url = \"http://www.presidency.ucsb.edu/satradio.php?year={}\".format(year)\n",
      "        html = requests.get(url).text\n",
      "        dom = web.Element(html)\n",
      "        # can't use find_urls because the URLs we want are incomplete :\\\n",
      "        for link in dom.by_tag('a'):\n",
      "            link = str(link)\n",
      "            if fnmatch(link, \"*pid*\"):\n",
      "                # pull out the PID\n",
      "                pid = re.findall(r'\\d+', link)[0]\n",
      "                saturday_addresses.append(get_document(base_url+pid))\n",
      "    \n",
      "    f = open('saturday-addresses.json', 'w')\n",
      "    f.write(json.dumps(saturday_addresses, indent=2))\n",
      "    f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# toggle to scrape and store Saturday Radio Address data\n",
      "# get_all_saturday_addresses()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in Saturday Radio Address data\n",
      "f = open('saturday-addresses.json')\n",
      "raw = f.read()\n",
      "saturday_addresses = json.loads(raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}